defaults:
  - _self_
  - optimizer: default.yaml
  - scheduler: cyclic.yaml

network_type: Transformer_conditional
_target_: twinturbo.src.models.twinturbo_model.TwinTURBO
latent_norm: False
latent_dim: 16
use_m_encodig: True
encoder_cfg: 
  transformer:
    _target_: mattstools.mattstools.simple_transformers.TransformerEncoder
    _partial_: true
    dim: 64
    num_layers: 4
    init_method: beit
    layer_config:
      ff_mult: 2
      num_heads: 8
  node_embd_config:
    hddn_dim: 64
    num_blocks: 1
    act_h: silu
    nrm: lyr
  outp_embd_config:
    hddn_dim: 64
    num_blocks: 1
    act_h: silu
    nrm: lyr
  ctxt_embd_config:
    hddn_dim: 64
    outp_dim: 16
    num_blocks: 1
    act_h: silu
    nrm: lyr
decoder_cfg:
  transformer:
    _target_: mattstools.mattstools.simple_transformers.TransformerEncoder
    _partial_: true
    dim: 64
    num_layers: 4
    init_method: beit
    layer_config:
      ff_mult: 2
      num_heads: 8
  node_embd_config:
    hddn_dim: 64
    num_blocks: 1
    act_h: silu
    nrm: lyr
  outp_embd_config:
    hddn_dim: 64
    num_blocks: 1
    act_h: silu
    nrm: lyr
  ctxt_embd_config:
    hddn_dim: 64
    outp_dim: 64
    num_blocks: 1
    act_h: silu
    nrm: lyr
loss_cfg:
  reco:
    w: 1

adversarial_cfg:
  mode: "double_discriminator_priority"
  discriminator:
    architecture:
      _target_: mattstools.mattstools.simple_transformers.FullEncoder
      _partial_: true
      outp_dim: 8
      transformer:
        _target_: mattstools.mattstools.simple_transformers.TransformerVectorEncoder
        _partial_: True
        dim: 32
        num_sa_layers: 4
        num_ca_layers: 2
        init_method: beit
        layer_config:
          ff_mult: 2
          num_heads: 2
      node_embd_config:
        hddn_dim: 32
        num_blocks: 1
        act_h: silu
      outp_embd_config:
        hddn_dim: 32
        num_blocks: 1
        act_h: silu
      ctxt_embd_config:
        hddn_dim: 32
        outp_dim: 8
        num_blocks: 1
        act_h: silu
    final_mlp:
      _target_: mattstools.mattstools.modules.DenseNetwork
      _partial_: true
      act_h: silu
      nrm: layer
      hddn_dim: 32
      num_blocks: 2
      act_o: "sigmoid"
  discriminator2:
    architecture:
      _target_: mattstools.mattstools.simple_transformers.FullEncoder
      _partial_: true
      outp_dim: 8
      transformer:
        _target_: mattstools.mattstools.simple_transformers.TransformerVectorEncoder
        _partial_: True
        dim: 32
        num_sa_layers: 4
        num_ca_layers: 2
        init_method: beit
        layer_config:
          ff_mult: 2
          num_heads: 2
      node_embd_config:
        hddn_dim: 32
        num_blocks: 1
        act_h: silu
      outp_embd_config:
        hddn_dim: 32
        num_blocks: 1
        act_h: silu
      ctxt_embd_config:
        hddn_dim: 32
        outp_dim: 8
        num_blocks: 1
        act_h: silu
    final_mlp:
      _target_: mattstools.mattstools.modules.DenseNetwork
      _partial_: true
      act_h: silu
      nrm: layer
      hddn_dim: 32
      num_blocks: 2
      act_o: "sigmoid"
  optimizer_main: default.yaml
  optimizer_d: default.yaml
  warmup: 5
  g_loss_weight: 2
  g_loss_gen_weight: 2
  every_n_steps_g: 1
  train_dis_in_warmup: False
  g_loss_weight_in_warmup: True
  loss_function: "binary_cross_entropy"
  scheduler: 
    scheduler_g:
      _target_: torch.optim.lr_scheduler.MultiStepLR
      _partial_: True
      milestones: [30, 100, 150, 200, 250]
      gamma: 0.5
    scheduler_d:
      _target_: torch.optim.lr_scheduler.MultiStepLR
      _partial_: True
      milestones: [30, 100, 150]
      gamma: 0.5
    scheduler_d2:
      _target_: torch.optim.lr_scheduler.MultiStepLR
      _partial_: True
      milestones: [30, 100, 150]
      gamma: 0.5
  gradient_clip_val: 0.1

valid_plots: False
add_standardizing_layer: True
# @package _global_

defaults:
  - _self_
  - data: gauss_corr_4_twinturbo_usem.yaml
  - step_train_template: twinturbo_dev
  - step_export_template: twinturbo
  - step_evaluate: twinturbo


do_train_template: False
do_export_template: False
do_train_cwola: False
do_evaluate_cwola: False
do_evaluation: True 


general:
  name: twinturbo_reco-cons-acontr-vic4
  seed: 12345 # For reproducibility
  run_dir: twinturbo/workspaces/test/${general.name}




# step_train_template:
#   project_name: twinTURBO_LHCO # Determines output directory path and wandb project
#   network_name: ${general.name}_${now:%Y-%m-%d}_${now:%H-%M-%S-%f} # Used for both saving and wandb
#   ckpt_path: null # Checkpoint path to resume training
#   precision: high # Should use medium if on ampere gpus
#   compile: null # Can set to default for faster compiles
#   full_resume: False
#   seed: ${general.seed} 

#   model:
#     _target_: twinturbo.src.models.twinturbo_model.TwinTURBO
#     encoder_mlp_config: 
#       hddn_dim: [64, 64, 64, 64]
#       act_h: "prelu"
#     decoder_mlp_config:
#       hddn_dim: [64, 64, 64, 64]
#       act_h: "prelu"
#     use_clip: False
#     latent_norm: True
#     var_group_list: [["x0"], ["x1"]]
#     latent_dim: 8
#     asim_alpha: 1.0
#     asim_beta: 1.0
#     l1_reg: 0.0e-7
#     loss_weights:
#       loss_reco: 1
#       loss_attractive: 0.0001
#       loss_repulsive: 0.0001
#       loss_back_vec: 0.5
#       loss_back_cont: 0.5
#     loss_balancing: 0
#     optimizer:
#       _target_: torch.optim.AdamW
#       _partial_: true
#       lr: 1.0e-4
#       weight_decay: 1.0e-5
#     scheduler:
#       scheduler:
#         _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#         _partial_: True
#         T_max: 100_000
#         last_epoch: -1
#         eta_min: 0
#       lightning:
#         monitor: valid/vall_loss
#         interval: step
#         frequency: 1

#   callbacks:
#     model_checkpoint:
#       _target_: pytorch_lightning.callbacks.ModelCheckpoint
#       dirpath: ${general.run_dir}/template/checkpoints
#       filename: best_{epoch:03d}
#       monitor: valid/total_loss
#       mode: min
#       save_last: True
#       auto_insert_metric_name: False

#     model_summary:
#       _target_: pytorch_lightning.callbacks.RichModelSummary
#       max_depth: 2

#     early_stopping:
#       _target_: pytorch_lightning.callbacks.EarlyStopping
#       monitor: valid/total_loss
#       mode: min
#       patience: 50
#       check_on_train_epoch_end: False

#     lr_monitor:
#       _target_: pytorch_lightning.callbacks.LearningRateMonitor
#       logging_interval: step

#   loggers:
#     wandb:
#       _target_: pytorch_lightning.loggers.wandb.WandbLogger
#       offline: True
#       id: null
#       log_model: False
#       project: ${step_train_template.project_name}
#       name: ${step_train_template.network_name}
#       save_dir: ${general.run_dir}
#       resume: ${step_train_template.full_resume}

#   data:
#     _target_: twinturbo.src.data.data.SimpleDataModule
#     train_frac: 0.8
#     train_data:
#       _target_: twinturbo.src.data.data.InMemoryDataFrameDict
#       list_order: ["data", "mass"]
#       file_path: "/home/users/o/oleksiyu/WORK/hyperproject/dummydata/gauss_corr_2.h5"
#       plotting_path: ${general.run_dir}/plots/train/
#       processor_cfg: 
#       - _target_: twinturbo.src.data.data.ProcessorSplitDataFrameVars
#         frame_name: "data"
#         new_df_dict: {"data": ["x0", "x1"], "mass": ["x0"]}
#       - _target_: twinturbo.src.data.data.ProcessorToFloat32
#         frame_names: ["data", "mass"]
#     loader_kwargs:
#       pin_memory: true
#       batch_size: 512
#       num_workers: 8 
#       drop_last: false

#   trainer:
#     _target_: pytorch_lightning.Trainer
#     min_epochs: 1
#     max_epochs: 99999
#     enable_progress_bar: True
#     accelerator: auto
#     devices: 1
#     gradient_clip_val: 5
#     precision: 32
#     check_val_every_n_epoch: 1

#     # Interpolated
#     default_root_dir: ${step_train_template.paths.full_path}

#   paths:
#     # output dir will be created by hydra
#     output_dir:  ${general.run_dir}/template
#     # Interpolated
#     root_dir: ${oc.env:PROJECT_ROOT}
#     full_path: ${general.run_dir}/template

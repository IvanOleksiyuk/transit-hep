# @package _global_

# Order indicates overwriting
defaults:
  - _self_
  - train_template: cathode
  - trainer: default
  - model: flow
  - data: lhco
  - loggers: csv
  - hydra: default
  - paths: default
  - callbacks: default


seed: 12345 # For reproducibility
project_name: transit # Determines output directory path and wandb project
network_name: ${now:%Y-%m-%d}_${now:%H-%M-%S-%f} # Used for both saving and wandb
ckpt_path: null # Checkpoint path to resume training

# Extra tweaks available with the new pytorch version
precision: high # Should use medium if on ampere gpus
compile: null # Can set to default for faster compiles

# COMPLETELY replaces the above config with what is contained in ${paths.full_path}
# This is ideal for resuming a job, log to the same directory
# Will also resume the loggers and set the ckpt_path to the latest
full_resume: False

run_dir: runs/test
do_train_template: True
do_export_template: True
do_cwola: True
do_evaluate_cwola: True

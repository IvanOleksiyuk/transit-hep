# @package _global_

do_train_template: True
do_export_template: False
do_train_cwola: False
do_evaluate_cwola: False
do_evaluation: True



general:
  name: twinTURBO_expn1_VIC
  seed: 12345 # For reproducibility
  run_dir: twinturbo/workspaces/exp_16_05_2024/${general.name}

train_template:
  project_name: twinTURBO_LHCO # Determines output directory path and wandb project
  network_name: ${general.name}_${now:%Y-%m-%d}_${now:%H-%M-%S-%f} # Used for both saving and wandb
  ckpt_path: null # Checkpoint path to resume training
  precision: high # Should use medium if on ampere gpus
  compile: null # Can set to default for faster compiles
  full_resume: False
  seed: ${general.seed} 
  data:
    _target_: twinturbo.src.data.lhco_simple.LHCODataModule
    train_frac: 0.8
    train_data_conf:
      file_path: "/home/users/o/oleksiyu/scratch/DATA/LHCO/events_anomalydetection_v2.features.h5"
      preprocessing_cfg: null
      var_group_list: [["m_n"], ["m_jj"], ["is_signal"]]
      to_return_list: [True, True, False]
      scaler_cfg: 
        scalers: ["norm", "norm", ""]
        selection_cfg: 
          n_inject_signal: 0
          cuts: ["m_jj > 2800", "m_jj < 5000"]
      selection_cfg: 
        n_inject_signal: 0
        cuts: ["m_jj > 2800", "m_jj < 5000"]
      plot_before_scale:
        save_path: ${general.run_dir}/plots/train_temp/
      plot_after_scale:
        save_path: ${general.run_dir}/plots/train_temp_skl/
    loader_kwargs:
      pin_memory: true
      batch_size: 512
      num_workers: 8 
      drop_last: false
  model:
    _target_: twinturbo.src.models.twinturbo_model.TwinTURBOVICreg
    use_clip: True
    var_group_list: [["m_n"], ["m_jj"], ["is_signal"]]
    latent_norm: True
    encoder_mlp_config: 
      hddn_dim: [64, 64, 64, 64]
      act_h: "prelu"
    decoder_mlp_config:
      hddn_dim: [64, 64, 64, 64]
      act_h: "prelu"
    latent_dim: 8
    asim_alpha: 1.0
    asim_beta: 1.0
    l1_reg: 0.0e-7
    loss_weights:
      loss_reco: 1
      loss_attractive: 0.0001
      loss_repulsive: 0.0001
      loss_back_vec: 0.5
      loss_back_cont: 0.5
    loss_balancing: 0
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 1.0e-4
      weight_decay: 1.0e-5
    scheduler:
      scheduler:
        _target_: torch.optim.lr_scheduler.CosineAnnealingLR
        _partial_: True
        T_max: 100_000
        last_epoch: -1
        eta_min: 0
      lightning:
        monitor: valid/vall_loss
        interval: step
        frequency: 1

  callbacks:
    model_checkpoint:
      _target_: pytorch_lightning.callbacks.ModelCheckpoint
      dirpath: ${general.run_dir}/template/checkpoints
      filename: best_{epoch:03d}
      monitor: valid/total_loss
      mode: min
      save_last: True
      auto_insert_metric_name: False

    model_summary:
      _target_: pytorch_lightning.callbacks.RichModelSummary
      max_depth: 2

    early_stopping:
      _target_: pytorch_lightning.callbacks.EarlyStopping
      monitor: valid/total_loss
      mode: min
      patience: 50
      check_on_train_epoch_end: False

    lr_monitor:
      _target_: pytorch_lightning.callbacks.LearningRateMonitor
      logging_interval: step

  loggers:
    wandb:
      _target_: pytorch_lightning.loggers.wandb.WandbLogger
      offline: True
      id: null
      log_model: False
      project: ${train_template.project_name}
      name: ${train_template.network_name}
      save_dir: ${general.run_dir}
      resume: ${train_template.full_resume}

  trainer:
    _target_: pytorch_lightning.Trainer
    min_epochs: 1
    max_epochs: 50
    enable_progress_bar: True
    accelerator: auto
    devices: 1
    gradient_clip_val: 5
    precision: 32
    check_val_every_n_epoch: 1
    default_root_dir: ${general.run_dir}

  paths:
    # output dir will be created by hydra
    output_dir:  ${general.run_dir}/template

    # Interpolated
    root_dir: ${oc.env:PROJECT_ROOT}
    full_path: ${general.run_dir}/template

#===================================================================================================
# Export_template
#===================================================================================================

export_template:
  seed: ${general.seed} 
  data:
    _target_: twinturbo.src.data.lhco_simple.LHCODataModule
    train_frac: 0.8
    loader_kwargs:
      pin_memory: true
      batch_size: 512
      num_workers: 8
      drop_last: false
    train_data_conf:
      dataset1_cfg:
        file_path: "/home/users/o/oleksiyu/scratch/DATA/LHCO/events_anomalydetection_v2.features.h5"
        preprocessing_cfg: null
        var_group_list: [["m_n"], ["m_jj"], ["is_signal"]]
        to_return_list: [True, False, False]
        scaler_cfg: 
          scalers: ["norm", "norm", ""]
          selection_cfg: 
            n_inject_signal: 0
            cuts: ["m_jj > 2800", "m_jj < 5000"]
        selection_cfg: 
          n_inject_signal: 0
          cuts: ["m_jj > 2800", "m_jj < 4000"]
          #n_bkg: 1000
        plot_before_scale:
          save_path: ${general.run_dir}/plots/train_temp_gen/
      dataset2_cfg:
        file_path: "/home/users/o/oleksiyu/scratch/DATA/LHCO/events_anomalydetection_v2.features.h5"
        preprocessing_cfg: null
        var_group_list: [["m_jj"], ["is_signal"]]
        to_return_list: [True, False]
        scaler_cfg: 
          scalers: ["norm", ""]
          selection_cfg: 
            n_inject_signal: 0
            cuts: ["m_jj > 2800", "m_jj < 5000"]
        selection_cfg: 
          n_inject_signal: 0
          cuts: ["m_jj > 4000", "m_jj < 5000"]
          #n_bkg: 1000
        plot_before_scale:
          save_path: ${general.run_dir}/plots/train_temp_gen/

  paths:
    # output dir will be created by hydra
    full_path:  ${general.run_dir}/template
    output_dir:  ${general.run_dir}/template
  
  get_best: false
  project_name: CATHODE_LHCO
  network_name: ${train_template.network_name}
  bands:
    - sr_signal
    - sr


#===================================================================================================
# CWOLA
#===================================================================================================
train_cwola:
  project_name: cwola # Determines output directory path and wandb project
  network_name: ${now:%Y-%m-%d}_${now:%H-%M-%S-%f} # Used for both saving and wandb
  ckpt_path: null # Checkpoint path to resume training
  precision: high # Should use medium if on ampere gpus
  compile: null # Can set to default for faster compiles
  full_resume: False
  seed: ${general.seed} 
  data:
    _target_: twinturbo.src.data.lhco_simple.LHCODataModule
    train_frac: 0.8
    train_data_conf:
      file_path: "/home/users/o/oleksiyu/scratch/DATA/LHCO/events_anomalydetection_v2.features.h5"
      preprocessing_cfg: null
      var_group_list: [["m_j1", "m_j2", "del_m", "tau21_j1", "tau32_j1", "tau21_j2", "tau32_j2", "m_n"],  ["m_jj"], ["is_signal"]]
      to_return_list: [True, True, False]
      scaler_cfg: 
        scalers: ["norm", "norm", "norm", "norm", "norm", "norm", "norm", "norm", "norm", ""]
        selection_cfg: 
          n_inject_signal: 0
          cuts: ["m_jj > 2800", "m_jj < 5000"]
      selection_cfg: 
        cuts: ["m_jj > 4000", "m_jj < 5000"]
        #n_sig: 90000
        #n_bkg: 90000
      plot_before_scale:
        save_path: ${general.run_dir}/plots/train/
      plot_after_scale:
        save_path: ${general.run_dir}/plots/train_skl/
    loader_kwargs:
      pin_memory: true
      batch_size: 512
      num_workers: 1
      drop_last: false
  
  model:
    _target_: twinturbo.src.models.MLPclassifier.MLPclassifier
    mlp_config:
      hddn_dim: [32, 32, 32]
    loss_name: crossentropy
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 1.0e-4
      weight_decay: 1.0e-5
    scheduler:
      scheduler:
        _target_: torch.optim.lr_scheduler.CosineAnnealingLR
        _partial_: True
        T_max: 100_000
        last_epoch: -1
        eta_min: 0
      lightning:
        monitor: valid/vall_loss
        interval: step
        frequency: 1
  
  callbacks:
    model_checkpoint:
      _target_: pytorch_lightning.callbacks.ModelCheckpoint
      dirpath: ${general.run_dir}/checkpoints
      filename: best_{epoch:03d}
      monitor: valid/total_loss
      mode: min
      save_last: True
      auto_insert_metric_name: False

    model_summary:
      _target_: pytorch_lightning.callbacks.RichModelSummary
      max_depth: 2

    early_stopping:
      _target_: pytorch_lightning.callbacks.EarlyStopping
      monitor: valid/total_loss
      mode: min
      patience: 50
      check_on_train_epoch_end: False

    lr_monitor:
      _target_: pytorch_lightning.callbacks.LearningRateMonitor
      logging_interval: step

  loggers:
    wandb:
      _target_: pytorch_lightning.loggers.wandb.WandbLogger
      offline: True
      id: null
      log_model: False
      project: ${train_cwola.project_name}
      name: ${train_cwola.network_name}
      save_dir: ${general.run_dir}
      resume: ${train_cwola.full_resume}

  trainer:
    _target_: pytorch_lightning.Trainer
    min_epochs: 1
    max_epochs: 99999
    enable_progress_bar: True
    accelerator: auto
    devices: 1
    gradient_clip_val: 5
    precision: 32
    check_val_every_n_epoch: 1
    default_root_dir: ${general.run_dir}

  paths:
    # output dir will be created by hydra
    output_dir:  ${general.run_dir}/cwola

    # Interpolated
    root_dir: ${oc.env:PROJECT_ROOT}
    full_path: ${general.run_dir}/cwola


export_cwola:
  placeholder_d: 4

evaluation:
  

